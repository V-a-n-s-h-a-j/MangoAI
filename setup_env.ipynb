{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "import os\n",
    "\n",
    "os.environ[\"MISTRAL_API_KEY\"] = \"vJ6LuLUeYqm1Uedzci7A2Fgh7tHnbS7p\"\n",
    "model = ChatMistralAI(model=\"mistral-large-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ciao!\\n\\nHere are a few other ways to say \"hi\" in Italian:\\n\\n* Salve! (Formal)\\n* Buongiorno! (Good morning/Good day)\\n* Buonasera! (Good evening)\\n* Ehi! (Hey!)\\n* Ciao a tutti! (Hi everyone!)', additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 15, 'total_tokens': 88, 'completion_tokens': 73}, 'model': 'mistral-large-latest', 'finish_reason': 'stop'}, id='run-6e1b0106-20ed-48f5-8fff-b7f3ba712e5e-0', usage_metadata={'input_tokens': 15, 'output_tokens': 73, 'total_tokens': 88})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the following from English into Italian\"),\n",
    "    HumanMessage(\"hi!\"),\n",
    "]\n",
    "\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"], stop=[\"\\n\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter `stop` not yet supported (https://docs.mistral.ai/api)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Bob! Nice to meet you. How are you today? Let's have a friendly conversation. ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "query = \"Hi! I'm Bob.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()  # output contains all messages in state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter `stop` not yet supported (https://docs.mistral.ai/api)\n"
     ]
    }
   ],
   "source": [
    "result = app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What did I just ask you?\")]},\n",
    "    config={\"configurable\": {\"thread_id\": \"abc123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You didn't ask me anything yet, Bob! You just said hi and introduced yourself. How about I start? What's your favorite hobby, Bob?\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "# setup_done = False\n",
    "# def setup():\n",
    "os.environ[\"MISTRAL_API_KEY\"] = \"vJ6LuLUeYqm1Uedzci7A2Fgh7tHnbS7p\"\n",
    "model = ChatMistralAI(model=\"mistral-large-latest\")\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    # response = model.invoke(state[\"messages\"], stop=[\"\\n\"])\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "# setup_done = True\n",
    "    # return app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def response(input_query, thread_id):\n",
    "    # app = setup()\n",
    "    config = {\"configurable\": {\"thread_id\": str(thread_id)}}\n",
    "    query = input_query\n",
    "\n",
    "    input_messages = [HumanMessage(query)]\n",
    "    output = await app.invoke({\"messages\": input_messages}, config)\n",
    "    return output[\"messages\"][-1].pretty_print()  # output contains all messages in state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Bob! Nice to meet you. How are you today? Let's have a friendly conversation. ðŸ˜Š How can I assist you or what would you like to talk about?\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(response(\"my names bob\", 'abc123'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You told me your name is Bob! Is that correct?\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(response(\"whats my name\", 'abc123'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Checkpointer requires one or more of the following 'configurable' keys: ['thread_id', 'checkpoint_ns', 'checkpoint_id']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[102], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhats is python\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m input_messages \u001b[38;5;241m=\u001b[39m [HumanMessage(query)]\n\u001b[1;32m----> 4\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_messages\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpretty_print()  \u001b[38;5;66;03m# output contains all messages in state\u001b[39;00m\n",
      "File \u001b[1;32md:\\StudyMaterial\\MangoAI-FASTAPI\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1961\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[0;32m   1959\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1960\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 1961\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1962\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1965\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1966\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1967\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1969\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1970\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1971\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "File \u001b[1;32md:\\StudyMaterial\\MangoAI-FASTAPI\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1590\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[0;32m   1574\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m   1575\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1576\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1577\u001b[0m     name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_name()),\n\u001b[0;32m   1578\u001b[0m     run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1579\u001b[0m )\n\u001b[0;32m   1580\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1581\u001b[0m     \u001b[38;5;66;03m# assign defaults\u001b[39;00m\n\u001b[0;32m   1582\u001b[0m     (\n\u001b[0;32m   1583\u001b[0m         debug,\n\u001b[0;32m   1584\u001b[0m         stream_modes,\n\u001b[0;32m   1585\u001b[0m         output_keys,\n\u001b[0;32m   1586\u001b[0m         interrupt_before_,\n\u001b[0;32m   1587\u001b[0m         interrupt_after_,\n\u001b[0;32m   1588\u001b[0m         checkpointer,\n\u001b[0;32m   1589\u001b[0m         store,\n\u001b[1;32m-> 1590\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_defaults\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1597\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# set up subgraph checkpointing\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpointer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32md:\\StudyMaterial\\MangoAI-FASTAPI\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1458\u001b[0m, in \u001b[0;36mPregel._defaults\u001b[1;34m(self, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[0;32m   1456\u001b[0m     checkpointer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpointer\n\u001b[0;32m   1457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpointer \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mget(CONF):\n\u001b[1;32m-> 1458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1459\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpointer requires one or more of the following \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[s\u001b[38;5;241m.\u001b[39mid\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39ms\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mcheckpointer\u001b[38;5;241m.\u001b[39mconfig_specs]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1460\u001b[0m     )\n\u001b[0;32m   1461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m CONFIG_KEY_STORE \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mget(CONF, {}):\n\u001b[0;32m   1462\u001b[0m     store: Optional[BaseStore] \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_STORE]\n",
      "\u001b[1;31mValueError\u001b[0m: Checkpointer requires one or more of the following 'configurable' keys: ['thread_id', 'checkpoint_ns', 'checkpoint_id']"
     ]
    }
   ],
   "source": [
    "query = \"whats is python\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = await app.invoke({\"messages\": input_messages})\n",
    "return output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MISTRAL_API_KEY\"] = \"vJ6LuLUeYqm1Uedzci7A2Fgh7tHnbS7p\"\n",
    "model = ChatMistralAI(model=\"mistral-large-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(query):\n",
    "    messages = [\n",
    "        # SystemMessage(\"Translate the following from English into Italian\"),\n",
    "        HumanMessage(query),\n",
    "    ]\n",
    "\n",
    "    result = model.invoke(messages)\n",
    "    return result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is a high-level, interpreted programming language known for its readability and simplicity. It was created by Guido van Rossum and first released in 1991. Python emphasizes code readability and allows developers to express concepts in fewer lines of code compared to languages like C++ or Java. Here are some key features of Python:\n",
      "\n",
      "1. **Ease of Learning and Readability**: Python's syntax is clean and easy to understand, making it a great choice for beginners and experienced developers alike.\n",
      "\n",
      "2. **Interpreted Language**: Python code is executed line by line, which makes debugging easier and allows for dynamic typing.\n",
      "\n",
      "3. **Extensive Standard Library**: Python comes with a vast standard library that supports many common programming tasks, such as file I/O, system calls, and even web browsers.\n",
      "\n",
      "4. **Third-Party Modules**: Python has a rich ecosystem of third-party libraries and frameworks, such as NumPy for numerical computations, pandas for data manipulation, and Django for web development.\n",
      "\n",
      "5. **Cross-Platform**: Python is available on various platforms including Windows, macOS, and Linux.\n",
      "\n",
      "6. **Community Support**: Python has a large and active community, which means plenty of tutorials, documentation, and forums to help you solve problems.\n",
      "\n",
      "7. **Versatility**: Python is used in a wide range of applications, including web development, data science, artificial intelligence, scripting, and automation.\n",
      "\n",
      "8. **Dynamic Typing**: Python uses dynamic typing, meaning that the type of a variable is determined at runtime, not in advance.\n",
      "\n",
      "Here is a simple example of a Python program that prints \"Hello, World!\":\n",
      "\n",
      "```python\n",
      "print(\"Hello, World!\")\n",
      "```\n",
      "\n",
      "Python's versatility and ease of use make it a popular choice for developers in various fields, from web development to scientific computing.\n"
     ]
    }
   ],
   "source": [
    "print(query('whats python?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = [1, 2, 3]\n",
    "A, B, _ = abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mC\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'C' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "# from langchain.vectorstores import Pinecone\n",
    "# import pinecone  # Pinecone Python client\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\StudyMaterial\\MangoAI-FASTAPI\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\StudyMaterial\\MangoAI-FASTAPI\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\HP\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "embeddings= HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# loader = TextLoader(\"../../modules/state_of_the_union.txt\")\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# documents = loader.load()\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m docs \u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m----> 5\u001b[0m db \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\StudyMaterial\\MangoAI-FASTAPI\\.venv\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:843\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[1;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[0;32m    840\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[0;32m    841\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ids\n\u001b[1;32m--> 843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\StudyMaterial\\MangoAI-FASTAPI\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:1044\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[0;32m   1026\u001b[0m \n\u001b[0;32m   1027\u001b[0m \u001b[38;5;124;03mThis is a user friendly interface that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;124;03m        faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m embedding\u001b[38;5;241m.\u001b[39membed_documents(texts)\n\u001b[1;32m-> 1044\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__from\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\StudyMaterial\\MangoAI-FASTAPI\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:1001\u001b[0m, in \u001b[0;36mFAISS.__from\u001b[1;34m(cls, texts, embeddings, embedding, metadatas, ids, normalize_L2, distance_strategy, **kwargs)\u001b[0m\n\u001b[0;32m    998\u001b[0m     index \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexFlatIP(\u001b[38;5;28mlen\u001b[39m(embeddings[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m    999\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1000\u001b[0m     \u001b[38;5;66;03m# Default to L2, currently other metric types not initialized.\u001b[39;00m\n\u001b[1;32m-> 1001\u001b[0m     index \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexFlatL2(\u001b[38;5;28mlen\u001b[39m(\u001b[43membeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m))\n\u001b[0;32m   1002\u001b[0m docstore \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocstore\u001b[39m\u001b[38;5;124m\"\u001b[39m, InMemoryDocstore())\n\u001b[0;32m   1003\u001b[0m index_to_docstore_id \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex_to_docstore_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# loader = TextLoader(\"../../modules/state_of_the_union.txt\")\n",
    "# documents = loader.load()\n",
    "# text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs =[]\n",
    "db = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_type='similarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"MISTRAL_API_KEY\"] = \"vJ6LuLUeYqm1Uedzci7A2Fgh7tHnbS7p\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the memory\n",
    "memory = VectorStoreRetrieverMemory(retriever=vectorstore.as_retriever())\n",
    "\n",
    "# Create a LangChain LLM chain\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "llm = ChatMistralAI(model=\"mistral-large-latest\")\n",
    "chain = ConversationChain(llm=llm, memory=memory, verbose=True) # verbose=True for debugging\n",
    "\n",
    "# Example interaction\n",
    "result1 = chain.predict(input=\"Hi, I'm a new user.\")\n",
    "print(result1)\n",
    "\n",
    "result2 = chain.predict(input=\"What was my first message?\")  # Memory retrieval!\n",
    "print(result2)\n",
    "\n",
    "result3 = chain.predict(input=\"My name is Alice.\")\n",
    "print(result3)\n",
    "\n",
    "result4 = chain.predict(input=\"So, what is my name?\") # Memory retrieval!\n",
    "print(result4)\n",
    "\n",
    "\n",
    "# To clear the memory (if needed)\n",
    "memory.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
